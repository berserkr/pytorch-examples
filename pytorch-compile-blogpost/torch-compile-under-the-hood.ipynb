{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d1d55d-42e3-4445-b6c1-dfd95f7999a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf torch_compile_debug\n",
    "!rm *.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b09805-e61c-4be4-aaf0-2dc148b0b736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch._dynamo\n",
    "from torchvision import models\n",
    "from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c7031-33d1-44ee-8398-3137b5f7d085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return torch.sin(x)**2 + torch.cos(x)**2\n",
    "\n",
    "md('''\n",
    "# $ y = f(x) = sin^2(x) + cos^2(x)$\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82568cbb-750b-4ed9-baf2-1cf2c6028c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md('''\n",
    "## Optimization problem:\n",
    "### find $w^*$ that $\\displaystyle \\min_{w} f(w)$\n",
    "\n",
    "## Gradient update\n",
    "### $ w_{i+1} = w_{i} - g(\\\\nabla f(w))$\n",
    "## For SGD\n",
    "### $  g(\\\\nabla f(w)) = \\\\alpha*\\\\nabla f(w)$\n",
    "## Which makes the update for SGD:\n",
    "### $ w_{i+1} = w_{i} - \\\\alpha*\\\\nabla f(w)$\n",
    "\n",
    "## Loss function\n",
    "### $loss(w): loss(model(w,batch_{inputs}), batch_{outputs})$ \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4c1717-2fbe-43f6-86ad-1f12d397d33d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md('''\n",
    "# **Forward graph:** $f(x) = sin^2(x)+cos^2(x)$ \\n\n",
    "# **Backward graph:** $\\\\frac {df(x)}{d\\\\vec{w}} = f\\'(x) = 2sin(x)cos(x) + 2cos(x)(-sin(x))$\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba8ff94-c457-41fd-b7ff-91ac36872bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "torch.nn.functional.mse_loss(f(x),torch.ones_like(x)) < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f80ceb-7e1a-4140-a17c-b6827e6e7461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "\n",
    "compiled_f = torch.compile(f)\n",
    "torch.nn.functional.mse_loss(compiled_f(x),torch.ones_like(x)) < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9cf8b-37ef-4b00-9c1b-d21e9c0ed6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inspect_backend(gm, sample_inputs):\n",
    "    code = gm.print_readable()\n",
    "    with open(\"forward.svg\", \"wb\") as file:\n",
    "        file.write(FxGraphDrawer(gm,'f').get_dot_graph().create_svg())\n",
    "    return gm.forward\n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_f = torch.compile(f, backend=inspect_backend)\n",
    "\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "out = compiled_f(x)\n",
    "\n",
    "md(f'''\n",
    "### Graph\n",
    "![]({'forward.svg'})\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2518d-1024-4aeb-a991-67de9dc51090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "619928ed-ed3f-45a4-8ef0-783c70bafaed",
   "metadata": {},
   "source": [
    "# AOTAutograd and Aten IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900c2c1-b83e-44ba-8eb2-c3d71c3bd53e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
    "from functorch.compile import make_boxed_func\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "\n",
    "def f(x):\n",
    "    return torch.sin(x)**2 + torch.cos(x)**2\n",
    "\n",
    "def inspect_backend(gm, sample_inputs): \n",
    "    # Forward compiler capture\n",
    "    def fw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        g = FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"forward_aot.svg\", \"wb\") as file:\n",
    "            file.write(g.get_dot_graph().create_svg())\n",
    "        return make_boxed_func(gm.forward)\n",
    "    \n",
    "    # Backward compiler capture\n",
    "    def bw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        g = FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"backward_aot.svg\", \"wb\") as file:\n",
    "            file.write(g.get_dot_graph().create_svg())\n",
    "        return make_boxed_func(gm.forward)\n",
    "    \n",
    "    # Call AOTAutograd\n",
    "    gm_forward = aot_module_simplified(gm,sample_inputs,\n",
    "                                       fw_compiler=fw,\n",
    "                                       bw_compiler=bw)\n",
    "\n",
    "    return gm_forward\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "y = torch.ones_like(x)\n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_f = torch.compile(f, backend=inspect_backend)\n",
    "out = torch.nn.functional.mse_loss(compiled_f(x), y).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ad8cc-6fc2-4e58-98c7-3dcff496ab59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md(f'''\n",
    "|![]({'forward_aot.svg'}) | < Forward graph <br><br><br> Backward graph >|![]({'backward_aot.svg'})|\n",
    "|---|---|---|\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a4f76f-1461-43d8-a8d4-1d24a7f6d1a8",
   "metadata": {},
   "source": [
    "# Decomposition to Core Aten IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f0343-65cc-4d7e-8c53-55292b46c87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
    "from functorch.compile import make_boxed_func\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "from torch._decomp import core_aten_decompositions\n",
    "\n",
    "def f_loss(x, y):\n",
    "    f_x = torch.sin(x)**2 + torch.cos(x)**2\n",
    "    return torch.nn.functional.mse_loss(f_x, y)\n",
    "\n",
    "# decompositions = core_aten_decompositions() # Use decomposition to Core Aten IR\n",
    "decompositions = {} # Don't use decomposition to Core Aten IR\n",
    "\n",
    "def inspect_backend(gm, sample_inputs): \n",
    "    def fw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        g = FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"forward_decomp.svg\", \"wb\") as file:\n",
    "            file.write(g.get_dot_graph().create_svg())\n",
    "        return make_boxed_func(gm.forward)\n",
    "    \n",
    "    def bw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        g = FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"backward_decomp.svg\", \"wb\") as file:\n",
    "            file.write(g.get_dot_graph().create_svg())\n",
    "        return make_boxed_func(gm.forward)\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        fw_compiler=fw,\n",
    "        bw_compiler=bw,\n",
    "        decompositions=decompositions\n",
    "    )\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "y = torch.ones_like(x)\n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_f = torch.compile(f_loss, backend=inspect_backend)\n",
    "out = compiled_f(x,y).backward()\n",
    "\n",
    "\n",
    "md('''\n",
    "# $MSE = (\\\\frac{1}{n})(\\\\vec{y}-\\\\vec{x})^2$\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb86a77-d050-4f80-96b0-c23ac12bd1ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md(f'''\n",
    "|![]({'forward_decomp.svg'}) | < Forward graph <br><br><br> Backward graph >|![]({'backward_decomp.svg'})|\n",
    "|---|---|---|\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a397ac-69a2-4cbe-af0d-9d3c50e00ece",
   "metadata": {},
   "source": [
    "# Decomposition to prim IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e71d40-6815-4167-862e-50253913e168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
    "from functorch.compile import make_boxed_func\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "from torch._decomp import core_aten_decompositions\n",
    "\n",
    "def f_loss(x, y):\n",
    "    f_x = torch.sin(x)**2 + torch.cos(x)**2\n",
    "    return torch.nn.functional.mse_loss(f_x, y)\n",
    "\n",
    "decompositions = core_aten_decompositions()\n",
    "decompositions.update(\n",
    "    torch._decomp.get_decompositions([\n",
    "        torch.ops.aten.sin,\n",
    "        torch.ops.aten.cos,\n",
    "        torch.ops.aten.add,\n",
    "        torch.ops.aten.sub,\n",
    "        torch.ops.aten.mul,\n",
    "        torch.ops.aten.sum,\n",
    "        torch.ops.aten.mean,\n",
    "        torch.ops.aten.pow.Tensor_Scalar,\n",
    "    ])\n",
    ")\n",
    "\n",
    "def inspect_backend(gm, sample_inputs): \n",
    "    def fw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        g = FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"forward_decomp_prims.svg\", \"wb\") as f:\n",
    "            f.write(g.get_dot_graph().create_svg())\n",
    "        return make_boxed_func(gm.forward)\n",
    "    \n",
    "    def bw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        g = FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"backward_decomp_prims.svg\", \"wb\") as f:\n",
    "            f.write(g.get_dot_graph().create_svg())\n",
    "        return make_boxed_func(gm.forward)\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        fw_compiler=fw,\n",
    "        bw_compiler=bw,\n",
    "        decompositions=decompositions\n",
    "    )\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "y = torch.ones_like(x)\n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_f = torch.compile(f_loss, backend=inspect_backend)\n",
    "out = compiled_f(x,y).backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b79bad-edc7-413d-94be-84ba286841b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md(f'''\n",
    "|![]({'forward_decomp_prims.svg'}) | < Forward graph <br><br><br> Backward graph >|![]({'backward_decomp_prims.svg'})|\n",
    "|---|---|---|\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff887d-46de-48d0-85a6-f1888c04e136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return torch.sin(x)**2 + torch.cos(x)**2 \n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_f = torch.compile(f, backend='inductor',\n",
    "                              options={'trace.enabled':True,\n",
    "                                       'trace.graph_diagram':True})\n",
    "\n",
    "\n",
    "# device = 'cpu'\n",
    "device = 'cuda'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "y = torch.ones_like(x)\n",
    "\n",
    "out = torch.nn.functional.mse_loss(compiled_f(x),y).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06362e9-7783-46bf-8498-bce83bc749ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "fwd = glob.glob('torch_compile_debug/run_*/aot_torchinductor/*forward*/graph_diagram.svg')[-1]\n",
    "bwd = glob.glob('torch_compile_debug/run_*/aot_torchinductor/*backward*/graph_diagram.svg')[-1]\n",
    "\n",
    "md(f'''\n",
    "|![]({fwd}) | < Forward graph <br><br><br> Backward graph >|![]({bwd})|\n",
    "|---|---|---|\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68e773-dce1-431e-8070-e2825d255bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
