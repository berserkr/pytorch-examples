{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5bea053-16e0-48cd-8e5a-f5472d1c65f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch                     \n",
    "import torch.nn as nn            \n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddafa68b-2a90-4ad0-889c-43c93c17b40e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf torch_compile_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b58f6a-1eef-47ec-9849-721858ef4dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, 6, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9100988-02c5-4a25-b77e-e7d207a38c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SimpleModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleModel, self).__init__()\n",
    "#         self.conv = nn.Conv2d(1, 6, 5)\n",
    "#         self.fc = nn.Linear(1176, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv(x))\n",
    "#         x = torch.flatten(x,1)\n",
    "#         x = F.relu(self.fc(x))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bbd41f6-ddc6-4506-bbcf-b37c6bfa1056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-15 20:03:01,980] torch._inductor.debug: [WARNING] model__2_forward_3 debug trace: /pytorch-examples/pytorch-torchcompile/torch_compile_debug/run_2023_03_15_20_01_10_252039-pid_373/aot_torchinductor/model__2_forward_3.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing FX graph to file: /pytorch-examples/pytorch-torchcompile/torch_compile_debug/run_2023_03_15_20_01_10_252039-pid_373/aot_torchinductor/model__2_forward_3.2/graph_diagram.svg\n",
      "tensor([[ 0.7080, -0.4106, -0.7436,  ...,  0.4363,  0.2918, -0.6914]],\n",
      "       device='cuda:0', grad_fn=<AsStridedBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = SimpleModel().to(device)\n",
    "cmodel = torch.compile(model, backend=\"inductor\", \n",
    "                       options={'trace.graph_diagram':True,\n",
    "                                'trace.enabled':True})\n",
    "\n",
    "input_tensor = torch.randn(1, 1, 32, 32).to(device)\n",
    "out = cmodel(input_tensor)\n",
    "print(out)\n",
    "\n",
    "from torch.fx import passes, symbolic_trace\n",
    "model_trace = symbolic_trace(model)\n",
    "\n",
    "g = passes.graph_drawer.FxGraphDrawer(model_trace, 'fn')\n",
    "with open(\"model_graph.svg\", \"wb\") as f:\n",
    "    f.write(g.get_dot_graph().create_svg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d65bc4-d30d-4986-ae0e-987ec21af4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "\n",
    "def toy_backend(gm, sample_inputs): \n",
    "    def my_compiler(gm, sample_inputs):\n",
    "        from torch.fx import passes, symbolic_trace\n",
    "        g = passes.graph_drawer.FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"biscuit.svg\", \"wb\") as f:\n",
    "            f.write(g.get_dot_graph().create_svg())\n",
    "\n",
    "        return gm.forward\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        fw_compiler=my_compiler\n",
    "    )\n",
    "\n",
    "torch._dynamo.reset()\n",
    "model = SimpleModel().to(device)\n",
    "input_tensor = torch.randn(1, 1, 32, 32).to(device)\n",
    "cmodel = torch.compile(model, backend=toy_backend)\n",
    "out = cmodel(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa96a7a7-3fc4-4335-8ec7-77ad880b8963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1176])\n",
      "[{Guard(name='torch', source=<GuardSource.GLOBAL: 1>, create_fn=<function GuardBuilder.FUNCTION_MATCH at 0x7f8ef5efbeb0>, is_volatile=False, guard_types=None, code_list=None, obj_weakref=None, guarded_class_weakref=None), Guard(name='self.conv', source=<GuardSource.LOCAL_NN_MODULE: 2>, create_fn=<function GuardBuilder.NN_MODULE at 0x7f8ef5efbe20>, is_volatile=False, guard_types=None, code_list=None, obj_weakref=None, guarded_class_weakref=None), Guard(name='self', source=<GuardSource.LOCAL: 0>, create_fn=<function GuardBuilder.NN_MODULE at 0x7f8ef5efbe20>, is_volatile=False, guard_types=['ID_MATCH'], code_list=['___check_obj_id(self, 140251669772896)'], obj_weakref=<weakref at 0x7f8ee2cf3f60; to 'SimpleModel' at 0x7f8ee2f4ba60>, guarded_class_weakref=<weakref at 0x7f8ee12a2340; to 'type' at 0x2f97330 (SimpleModel)>), Guard(name='print', source=<GuardSource.GLOBAL: 1>, create_fn=<function GuardBuilder.BUILTIN_MATCH at 0x7f8ef5efbf40>, is_volatile=False, guard_types=None, code_list=None, obj_weakref=None, guarded_class_weakref=None), Guard(name='F', source=<GuardSource.GLOBAL: 1>, create_fn=<function GuardBuilder.FUNCTION_MATCH at 0x7f8ef5efbeb0>, is_volatile=False, guard_types=None, code_list=None, obj_weakref=None, guarded_class_weakref=None), Guard(name='x', source=<GuardSource.LOCAL: 0>, create_fn=<function GuardBuilder.TENSOR_MATCH at 0x7f8ef5f205e0>, is_volatile=False, guard_types=None, code_list=None, obj_weakref=None, guarded_class_weakref=None)}, {Guard(name='self', source=<GuardSource.LOCAL: 0>, create_fn=<function GuardBuilder.NN_MODULE at 0x7f8ef5efbe20>, is_volatile=False, guard_types=['ID_MATCH'], code_list=['___check_obj_id(self, 140251669772896)'], obj_weakref=<weakref at 0x7f8ee2cf3f60; to 'SimpleModel' at 0x7f8ee2f4ba60>, guarded_class_weakref=<weakref at 0x7f8ee12a2340; to 'type' at 0x2f97330 (SimpleModel)>), Guard(name='F', source=<GuardSource.GLOBAL: 1>, create_fn=<function GuardBuilder.FUNCTION_MATCH at 0x7f8ef5efbeb0>, is_volatile=False, guard_types=None, code_list=None, obj_weakref=None, guarded_class_weakref=None), Guard(name='self.fc', source=<GuardSource.LOCAL_NN_MODULE: 2>, create_fn=<function GuardBuilder.NN_MODULE at 0x7f8ef5efbe20>, is_volatile=False, guard_types=None, code_list=None, obj_weakref=None, guarded_class_weakref=None), Guard(name='x', source=<GuardSource.LOCAL: 0>, create_fn=<function GuardBuilder.TENSOR_MATCH at 0x7f8ef5f205e0>, is_volatile=False, guard_types=None, code_list=None, obj_weakref=None, guarded_class_weakref=None)}]\n"
     ]
    }
   ],
   "source": [
    "e = torch._dynamo.explain(model, input_tensor)\n",
    "print(e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95c881f7-b6e2-4bbc-a7e9-386ab9c1e8e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.fx import passes, symbolic_trace\n",
    "model_trace = symbolic_trace(model)\n",
    "\n",
    "g = passes.graph_drawer.FxGraphDrawer(model_trace, 'fn')\n",
    "with open(\"asdf_graph.svg\", \"wb\") as f:\n",
    "    f.write(g.get_dot_graph().create_svg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d766f129-9bcd-4b90-bd99-4e26892a8d29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000704\n"
     ]
    }
   ],
   "source": [
    "from ctypes import c_void_p, c_long\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "from torch import empty_strided, as_strided, device\n",
    "from torch._inductor.codecache import AsyncCompile\n",
    "from torch._inductor.select_algorithm import extern_kernels\n",
    "\n",
    "aten = torch.ops.aten\n",
    "assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
    "async_compile = AsyncCompile()\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from torch._inductor.triton_ops.autotune import grid, start_graph, end_graph\n",
    "from torch._C import _cuda_getCurrentRawStream as get_cuda_stream\n",
    "\n",
    "\n",
    "triton__0 = async_compile.triton('''\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from torch._inductor.ir import ReductionHint\n",
    "from torch._inductor.ir import TileHint\n",
    "from torch._inductor.triton_ops.autotune import pointwise\n",
    "from torch._inductor.utils import instance_descriptor\n",
    "\n",
    "@pointwise(size_hints=[8192], filename=__file__, meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'constants': {}, 'mutated_arg_names': ['in_out_ptr0'], 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})\n",
    "@triton.jit\n",
    "def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n",
    "    xnumel = 4704\n",
    "    xoffset = tl.program_id(0) * XBLOCK\n",
    "    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n",
    "    xmask = xindex < xnumel\n",
    "    x2 = xindex\n",
    "    x1 = (xindex // 784)\n",
    "    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)\n",
    "    tmp1 = tl.load(in_ptr0 + (x1), xmask)\n",
    "    tmp2 = tmp0 + tmp1\n",
    "    tl.store(in_out_ptr0 + (x2 + tl.zeros([XBLOCK], tl.int32)), tmp2, xmask)\n",
    "''')\n",
    "\n",
    "\n",
    "async_compile.wait(globals())\n",
    "del async_compile\n",
    "\n",
    "def call(args):\n",
    "    primals_1, primals_2, primals_3 = args\n",
    "    args.clear()\n",
    "    with torch.cuda._DeviceGuard(0):\n",
    "        torch.cuda.set_device(0) # no-op to ensure context\n",
    "        buf0 = aten.convolution(primals_3, primals_1, None, (1, 1), (0, 0), (1, 1), False, (0, 0), 1)\n",
    "        assert_size_stride(buf0, (1, 6, 28, 28), (4704, 784, 28, 1))\n",
    "        buf1 = buf0; del buf0  # reuse\n",
    "        stream0 = get_cuda_stream(0)\n",
    "        triton__0.run(buf1, primals_2, 4704, grid=grid(4704), stream=stream0)\n",
    "        del primals_2\n",
    "        return (as_strided(buf1, (1, 4704), (4704, 1)), buf1, primals_1, primals_3, )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from torch._dynamo.testing import rand_strided\n",
    "    from torch._inductor.utils import print_performance\n",
    "    primals_1 = rand_strided((6, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)\n",
    "    primals_2 = rand_strided((6, ), (1, ), device='cuda:0', dtype=torch.float32)\n",
    "    primals_3 = rand_strided((1, 1, 32, 32), (1024, 1024, 32, 1), device='cuda:0', dtype=torch.float32)\n",
    "    print_performance(lambda: call([primals_1, primals_2, primals_3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c8d278c-7317-499c-9810-ae6eedbd2520",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._inductor.codecache.AsyncCompile at 0x7f2e8e8f12a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async_compile = AsyncCompile()\n",
    "async_compile.triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8c7c8-622d-4e05-b875-48cc53d045ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
