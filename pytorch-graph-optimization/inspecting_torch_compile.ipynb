{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c42e096-2d57-4e1d-976b-003cade9a05c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch._dynamo\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a16cbc2-0baa-44f4-beb1-030dcb7c1433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "\n",
    "batch_size = 8\n",
    "x = torch.randn(batch_size, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43846e32-0973-4e62-ab70-563ec780bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fx import passes, symbolic_trace\n",
    "model = symbolic_trace(fn)\n",
    "\n",
    "g = passes.graph_drawer.FxGraphDrawer(model, 'fn')\n",
    "with open(\"unoptimized_graph.svg\", \"wb\") as f:\n",
    "    f.write(g.get_dot_graph().create_svg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a7036c0-cf13-44fb-ac96-79ad76fad291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing FX graph to file: forward.svg\n",
      "Writing FX graph to file: backward.svg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1251: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1251: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "from functorch.compile import compiled_function, draw_graph\n",
    "\n",
    "\n",
    "def toy_backend(gm, sample_inputs): \n",
    "    def fw(gm, sample_inputs):\n",
    "        draw_graph(gm, \"forward.svg\")\n",
    "        return gm.forward\n",
    "    \n",
    "    def bw(gm, sample_inputs):\n",
    "        draw_graph(gm, \"backward.svg\")\n",
    "        return gm.forward\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        fw_compiler=fw,\n",
    "        bw_compiler=bw\n",
    "    )\n",
    "\n",
    "def fn(x):\n",
    "    return x**2\n",
    "\n",
    "model = fn\n",
    "x = torch.tensor(5., requires_grad=True)\n",
    "\n",
    "torch._dynamo.reset()\n",
    "cmodel = torch.compile(model, backend=toy_backend, dynamic=True)\n",
    "\n",
    "# triggers compilation of forward graph on the first run\n",
    "out = cmodel(x).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259b9c86-5f23-4f61-b178-15e71a6e2197",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    param = self.param\n",
      "    add = x + param;  x = param = None\n",
      "    linear = self.linear(add);  add = None\n",
      "    clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\n",
      "    return clamp\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(symtraced.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125d380-aaa2-4685-95a4-a035449cacfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
