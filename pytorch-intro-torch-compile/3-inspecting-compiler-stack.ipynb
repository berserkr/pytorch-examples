{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acdb00b0-7032-4133-9cd4-37dddc40a33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch._dynamo\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c8357d-f9a1-4a7f-be01-1aae1c8a76c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = x.luis_add(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "\n",
    "batch_size = 8\n",
    "input = torch.randn(batch_size, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e49cf111-80bc-4074-95a5-19f329d495d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026ae4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0a0+gitd70ddad\n",
      "tensor([[ 6.1200e-02,  6.1260e-01,  4.9810e-01, -8.8650e-02, -1.2920e-01,\n",
      "          2.2570e-02,  2.9933e-01,  1.0666e-01,  3.2522e-01,  9.4256e-01,\n",
      "          3.9640e-01,  1.2687e+00, -2.3221e-02,  1.3839e-01, -9.4085e-02,\n",
      "         -4.0286e-02, -1.5851e-01,  4.7221e-01,  4.5965e-01, -1.6994e-01,\n",
      "          6.4940e-01,  1.3770e+00, -4.4846e-02, -1.1805e-01,  4.3350e-01,\n",
      "          1.2265e-01, -5.2011e-02, -8.6966e-02,  8.9819e-01,  1.8202e+00,\n",
      "          4.9735e-01, -3.1347e-02, -1.4244e-01, -7.9711e-02,  2.9998e-01,\n",
      "         -1.1166e-01, -1.4799e-01,  2.9165e-02,  1.0200e+00,  3.6100e-01,\n",
      "         -1.2993e-01,  1.2405e+00, -9.4801e-02,  4.6645e-01, -1.5835e-01,\n",
      "          1.7928e+00,  3.1570e+00, -8.8806e-02,  3.2276e-02, -8.4513e-02,\n",
      "          2.2140e+00,  2.1544e-02, -4.7591e-02,  1.8657e+00,  2.5779e-01,\n",
      "         -8.6051e-02,  2.8156e-01,  1.0803e-01, -4.0472e-02,  6.8307e-01,\n",
      "          3.2200e+00,  7.2199e-01, -1.2213e-01, -8.5889e-04],\n",
      "        [-1.5169e-01,  2.2543e+00,  1.3876e+00, -1.2344e-01, -6.6499e-02,\n",
      "         -5.9463e-02, -1.4528e-01,  5.5305e-01,  1.9963e-01, -1.2284e-01,\n",
      "         -1.3058e-01,  5.9927e-01,  4.9412e-03,  1.9087e-01, -5.7116e-02,\n",
      "         -1.6201e-01, -1.6237e-01,  2.0389e+00,  2.8781e-02,  1.2257e+00,\n",
      "          1.1416e+00,  7.9778e-01, -1.4907e-01, -7.2629e-02,  3.0633e+00,\n",
      "         -2.0934e-02, -1.1997e-01,  1.5816e+00,  6.3131e-01,  2.3255e-01,\n",
      "          1.9812e-01,  1.9444e+00, -1.6989e-01,  5.9035e-01, -1.6702e-01,\n",
      "          1.2478e-01, -6.7239e-03, -1.6750e-02,  7.9273e-01,  5.4752e-01,\n",
      "         -1.6986e-01, -1.6936e-01, -2.8516e-02,  3.9220e-01, -1.6026e-01,\n",
      "          1.4093e+00, -1.1164e-01,  1.8705e-01, -1.2052e-01, -5.3149e-02,\n",
      "          1.3438e+00,  1.9154e-02,  6.2638e-01,  1.2383e+00,  8.0279e-01,\n",
      "          8.5464e-01,  4.8072e-01, -1.6102e-01, -2.5864e-02,  1.5688e-01,\n",
      "          3.4305e+00, -8.5597e-02,  2.7168e-01, -1.6996e-01],\n",
      "        [-1.4118e-01,  1.5122e-01, -1.5573e-01, -8.3605e-02, -5.1263e-02,\n",
      "          1.6241e+00,  6.9586e-01, -1.1214e-01,  9.8593e-02,  1.2547e+00,\n",
      "          3.0550e-01,  3.0962e+00, -5.2029e-02,  2.5650e-01, -1.6995e-01,\n",
      "         -4.6305e-02,  5.9241e-01,  5.0844e-01,  1.4374e-01,  3.8429e-01,\n",
      "          5.4542e-01,  6.9321e-01,  1.7753e+00,  8.2039e-01,  1.3221e+00,\n",
      "          5.1453e-02, -9.1832e-02,  1.4235e+00, -3.3323e-02,  9.5294e-01,\n",
      "         -1.6472e-01,  4.2133e-01, -1.6746e-01, -2.0867e-02, -7.6706e-02,\n",
      "          2.7011e+00, -2.8360e-03, -1.6143e-01, -6.7558e-02, -1.2045e-01,\n",
      "          2.9365e+00, -1.6463e-01,  1.6953e+00, -6.0533e-02, -1.0126e-01,\n",
      "         -1.6360e-01,  4.5343e-01, -9.5479e-02, -1.6963e-01,  9.4651e-01,\n",
      "         -1.0336e-01,  1.8796e-01,  1.7982e+00,  1.7276e+00,  1.6018e+00,\n",
      "         -1.6449e-01, -1.3481e-01, -1.6687e-01,  5.7861e-02, -4.4847e-02,\n",
      "         -1.2465e-01, -1.5155e-01, -6.7498e-02,  1.6146e+00],\n",
      "        [ 4.3714e-01,  3.3731e-01, -5.5140e-02,  4.4543e-02,  3.1146e-01,\n",
      "          6.6725e-02,  7.5976e-02, -3.6983e-02, -1.5031e-01, -1.6712e-01,\n",
      "          6.7138e-01, -1.2658e-02, -6.3587e-02, -1.5984e-01,  8.3728e-01,\n",
      "          4.6153e-01,  3.4728e+00,  6.8040e-01, -1.9139e-03,  1.8506e+00,\n",
      "          8.9040e-01,  2.6867e-02, -8.7932e-02,  1.4968e+00, -1.6745e-01,\n",
      "         -1.6894e-01,  1.5466e+00,  3.6640e-01,  2.4343e-01,  8.3783e-01,\n",
      "         -1.6494e-01, -1.6202e-01, -1.6516e-01, -1.3494e-01, -1.6814e-01,\n",
      "          3.0864e-01,  1.5920e-01, -9.7621e-02,  2.4480e+00,  1.0536e+00,\n",
      "          1.3576e+00,  6.6814e-01,  1.7598e-01, -1.6077e-01,  8.9217e-01,\n",
      "          9.7426e-01, -1.2396e-01, -1.6881e-01,  1.6743e+00, -5.1524e-02,\n",
      "          2.0479e+00,  9.5801e-01, -4.8338e-02,  3.7314e+00, -1.5857e-01,\n",
      "          2.3803e+00,  1.4779e-01, -2.1679e-02, -1.5961e-01,  5.2322e-01,\n",
      "         -1.6890e-01, -1.4939e-01,  1.3518e+00,  4.4259e-02],\n",
      "        [-1.6996e-01,  2.5670e-01, -8.9815e-02,  1.3096e+00, -1.0655e-01,\n",
      "         -1.0397e-01,  2.1524e+00, -1.3740e-01, -1.1526e-01,  1.8729e+00,\n",
      "          3.6843e-01,  1.0431e-01, -6.7227e-03,  9.2792e-01,  1.0689e+00,\n",
      "          1.7408e-01,  2.8950e-01,  9.3868e-01, -1.0803e-01,  2.1781e-02,\n",
      "          9.0450e-01, -1.6659e-01,  1.0601e+00,  3.0035e-01,  2.0146e-01,\n",
      "          2.1085e-01, -1.6974e-01, -1.3890e-01, -1.6627e-01, -1.6945e-01,\n",
      "          7.9158e-01,  1.2480e+00, -1.6931e-01,  1.0069e-01,  9.2114e-01,\n",
      "         -1.6996e-01, -1.3138e-01,  2.9096e-01,  9.1617e-01,  5.8646e-01,\n",
      "         -1.6327e-01, -7.4672e-03,  8.4000e-02, -1.3947e-02,  4.3831e-01,\n",
      "          1.4673e+00,  7.0492e-03, -1.4315e-01, -1.6119e-01,  2.9601e-01,\n",
      "         -1.6902e-01, -4.8266e-02, -1.4311e-01,  8.4436e-01,  6.1233e-01,\n",
      "          2.3295e-01,  1.2661e+00, -8.0895e-02, -2.9384e-02, -1.1533e-01,\n",
      "          8.0652e-01,  3.9735e-01,  9.2999e-02, -8.0660e-02],\n",
      "        [-1.2297e-01,  1.1726e+00,  1.8496e-01,  1.2431e+00,  9.2033e-01,\n",
      "         -6.4207e-02, -6.8517e-02,  1.1417e-01, -1.5500e-01,  1.6032e+00,\n",
      "          1.7678e+00,  2.0711e-01, -1.2181e-01,  1.3754e+00,  5.3627e-02,\n",
      "         -2.4538e-03, -1.0403e-01,  1.3311e+00,  1.3420e+00,  1.1841e+00,\n",
      "          1.4937e-01, -8.2003e-02,  3.3178e-01, -1.5627e-02,  4.3788e-01,\n",
      "         -1.6712e-01, -3.1313e-02, -1.6990e-01, -1.4681e-01, -1.6395e-01,\n",
      "          2.7902e-01,  1.7597e+00, -1.4230e-01, -4.5952e-02, -1.6745e-01,\n",
      "         -1.0028e-01,  7.3535e-01, -1.3591e-01, -1.4731e-01,  3.7972e-02,\n",
      "          1.6555e-01, -1.4623e-01, -1.6271e-01,  6.2161e-01, -1.5507e-01,\n",
      "          7.5781e-02, -1.4538e-01,  3.7277e-02, -1.1543e-01,  5.5588e-01,\n",
      "          2.0664e-01, -1.6866e-01, -2.1855e-02,  2.9414e-01,  1.0127e+00,\n",
      "         -1.6316e-01,  1.2040e+00,  5.7256e-01, -9.1044e-02,  1.6295e-01,\n",
      "          6.7027e-01, -1.5163e-01,  4.3239e-02, -1.3069e-01],\n",
      "        [ 1.0525e-01,  3.0696e-01, -1.0504e-01,  3.5720e-01,  1.4217e-02,\n",
      "          1.3386e-01, -7.6113e-02,  1.0662e-01, -1.6212e-01,  3.1538e-01,\n",
      "         -2.5610e-02,  6.4463e-01, -1.5333e-01, -1.1704e-01, -1.5230e-01,\n",
      "         -4.5243e-03,  9.3291e-02,  4.5455e-02, -6.2952e-02,  9.4592e-01,\n",
      "         -2.2494e-02,  8.3193e-01, -1.6032e-01,  9.3910e-02, -1.0346e-01,\n",
      "         -1.4427e-01,  3.3463e-01, -1.6866e-01,  1.5694e+00,  1.1228e-01,\n",
      "         -1.2604e-01, -1.6816e-01, -9.8457e-02, -9.9511e-02, -7.4414e-02,\n",
      "          6.8114e-01,  1.2439e+00, -1.6285e-01, -1.4709e-01, -1.3677e-01,\n",
      "          2.0483e+00,  3.3876e-01,  2.5588e-01,  1.0176e+00,  4.7997e-01,\n",
      "         -1.6367e-01, -1.4951e-01,  1.5789e+00,  3.6330e-02, -1.5426e-01,\n",
      "          3.1583e-01,  4.4738e-01,  6.3200e-02,  1.3833e+00,  8.0411e-01,\n",
      "          4.8804e-01, -9.7196e-02,  8.4740e-01, -1.6948e-01, -9.5835e-02,\n",
      "          7.2872e-02, -1.6892e-01,  1.0799e+00,  1.0685e+00],\n",
      "        [ 2.0625e+00,  3.8300e-02, -1.6286e-01,  2.2524e+00,  1.7874e+00,\n",
      "          2.7510e-01, -1.6162e-01, -1.3919e-01, -4.0177e-02,  1.9252e+00,\n",
      "          1.9913e+00, -1.6627e-01, -1.4998e-01, -1.3059e-01,  2.9959e+00,\n",
      "          1.7633e-01,  8.9685e-02,  2.4802e-01, -1.1018e-02,  1.0126e+00,\n",
      "         -8.0184e-02, -1.6679e-01,  5.6695e-01,  2.0223e-02, -1.5623e-01,\n",
      "         -1.6856e-01,  1.8224e+00, -1.3693e-01, -7.4673e-03, -6.6155e-03,\n",
      "         -1.5101e-01, -1.6763e-01, -7.6176e-03, -1.0364e-01, -1.6603e-01,\n",
      "         -1.1145e-01,  2.1829e+00, -2.0330e-02,  1.3060e-01, -6.1035e-02,\n",
      "          2.2539e-01, -6.6326e-02,  2.4577e+00,  1.8915e+00, -1.6886e-01,\n",
      "          1.6224e+00,  2.7414e-01, -1.4425e-01,  6.6014e-02,  1.6137e+00,\n",
      "         -1.5113e-01,  6.1487e-01, -1.3844e-01,  5.3395e-01,  7.9498e-02,\n",
      "          1.5113e-01,  2.0779e-01, -1.5887e-01, -1.6922e-01,  7.2672e-01,\n",
      "         -6.5770e-02, -1.6007e-01,  5.1813e-01, -1.6851e-01]],\n",
      "       grad_fn=<GeluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "out = model(input)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296572f4-0eeb-4ae8-9977-d9d24e1bb7a1",
   "metadata": {},
   "source": [
    "### Invoke `torch.compile` produces a fx graph in Torch IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a78f233e-7295-4e7b-9439-7903f3a5a3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamo produced a fx Graph in Torch IR:\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, L_x_ : torch.Tensor):\n",
      "        l_x_ = L_x_\n",
      "        \n",
      "        # File: /tmp/ipykernel_7672/678154743.py:7, code: x = self.fc1(x)\n",
      "        l__self___fc1 = self.L__self___fc1(l_x_);  l_x_ = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_7672/678154743.py:8, code: x = x.luis_add(x)\n",
      "        luis_add = l__self___fc1.luis_add(l__self___fc1);  l__self___fc1 = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_7672/678154743.py:9, code: x = torch.nn.functional.gelu(x)\n",
      "        gelu = torch._C._nn.gelu(luis_add);  luis_add = None\n",
      "        return (gelu,)\n",
      "        \n",
      "Notice that sample_inputs is a list of flattened FakeTensor:\n",
      "[tensor([[-0.1829,  0.5798, -0.9840,  0.8186, -1.6522,  0.4352, -0.6053,  0.1342,\n",
      "          0.8324,  0.3900, -1.8516,  0.2012,  0.3902, -1.2492,  1.0215,  1.0482,\n",
      "          2.3499, -1.7583,  0.9609,  0.8368,  1.7870,  0.6308,  0.9022, -0.9496,\n",
      "         -0.4872,  1.0258,  1.4198,  0.0269,  0.4601,  0.3273,  0.2634, -1.2817],\n",
      "        [-0.7448, -0.2411, -1.0432,  1.5816, -0.2868, -0.1612,  1.2694,  1.5341,\n",
      "          0.1774,  1.9471, -0.0271, -1.6446, -2.4979,  1.3045,  1.0969,  0.7295,\n",
      "          0.0701,  0.2731,  0.9148, -1.7291,  0.5367,  0.2540, -1.9897,  1.4262,\n",
      "         -0.7561,  0.8693,  1.7269, -0.5927,  0.2531, -0.4639,  0.3639, -0.9877],\n",
      "        [ 2.6077,  1.5331, -1.2125,  0.3107, -0.4650,  1.2305,  0.7844, -2.4670,\n",
      "          0.8438, -0.2076,  1.5323, -0.4504,  1.4532,  1.2367,  0.3800,  0.0279,\n",
      "          0.0670,  0.6240, -0.1717,  0.2350, -0.1070,  1.6282,  0.3301,  0.6686,\n",
      "          0.8214,  1.6679, -0.5634,  0.3892, -0.5196,  1.7042, -1.1265, -1.0026],\n",
      "        [ 0.7476,  0.2943,  0.9698,  0.4373, -0.4087,  0.3646, -0.7041,  1.6921,\n",
      "          0.3472, -1.5482,  0.8849,  1.1535, -1.2238, -0.5784,  2.1597,  0.3380,\n",
      "          0.8981,  0.6288, -1.8741,  0.9115, -1.2282, -0.2524, -1.6233,  0.3856,\n",
      "          1.4253, -2.0148,  0.4600, -1.6864,  0.1749,  0.3431,  1.3881,  0.1866],\n",
      "        [ 1.0355,  1.0776,  1.0172, -0.8334, -0.9903, -0.1086,  0.7378,  0.9435,\n",
      "         -2.0064, -1.2934,  1.1527, -0.2432, -0.8797,  0.6938, -0.7814, -0.3354,\n",
      "         -0.3802, -0.7251,  0.8197,  0.9148, -0.0922, -0.8227,  0.6341,  0.6859,\n",
      "         -1.8793,  0.3925, -0.4719,  0.4405,  1.1722, -0.4336,  1.2248, -1.3684],\n",
      "        [ 0.0287, -0.0320, -1.8951,  0.6547, -0.6742, -1.5632,  0.9507, -0.5725,\n",
      "          0.5775, -0.4084, -0.7747, -0.8379,  0.9847,  0.1914,  0.2799, -1.1330,\n",
      "          1.1216, -0.5333,  2.0509, -0.5840, -0.9239,  0.3860,  1.0055,  0.0378,\n",
      "         -0.8453,  1.2028,  0.7274, -0.9478,  0.9119, -1.5467, -0.1440, -1.1134],\n",
      "        [-0.0172,  0.3733, -1.4549, -0.0465,  0.1265,  1.4910, -0.3186,  0.8238,\n",
      "          1.0458,  0.2687, -1.4643,  0.1345,  1.6410, -0.2950,  0.1600, -0.5565,\n",
      "          0.3808, -0.7125,  0.7066, -0.5770, -1.5951,  0.4605, -0.7509, -0.4668,\n",
      "          0.5737,  0.0733,  0.5450,  0.6848, -0.6321,  0.0159,  0.4398,  0.4993],\n",
      "        [-1.0351, -0.2924,  1.1460,  0.9091,  0.9464,  0.6133,  1.1600, -1.2703,\n",
      "         -0.1984, -1.1082,  0.1089, -0.0319, -0.0583, -0.5617,  0.0908, -0.3814,\n",
      "         -0.2762,  2.9723, -0.0951,  1.8080, -1.4932, -0.3472,  0.0660, -0.8403,\n",
      "         -0.8957, -1.3059, -0.3951, -0.7850,  1.4620, -0.1753,  0.3565,  0.1124]])]\n"
     ]
    }
   ],
   "source": [
    "def toy_backend(gm, sample_inputs):\n",
    "    print(\"Dynamo produced a fx Graph in Torch IR:\")\n",
    "    gm.print_readable()\n",
    "\n",
    "    print(\"Notice that sample_inputs is a list of flattened FakeTensor:\")\n",
    "    print(sample_inputs)\n",
    "    return gm.forward\n",
    "\n",
    "torch._dynamo.reset()\n",
    "cmodel = torch.compile(model, backend=toy_backend, dynamic=True)\n",
    "\n",
    "# triggers compilation of forward graph on the first run\n",
    "out = cmodel(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d8eba-34d4-42a1-9914-32f428403042",
   "metadata": {},
   "source": [
    "## Invoke AOTAutograd, produces forward + backward FX graph in Aten IR\n",
    "* Captures forward + backwards\n",
    "* Lowering from Torch IR to Aten/Prims IR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68d6c5-f52a-471c-bff6-c5127cea5723",
   "metadata": {},
   "source": [
    "### Core Aten IR (https://pytorch.org/docs/master/ir.html#core-aten-ir)\n",
    "\n",
    "* A strict subset of aten operators (< 250) after decompositions\n",
    "* Purely functional (no inputs mutations）\n",
    "* Guaranteed metadata information, e.g. dtype and shape propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19a66032-61e8-4902-b4b2-056cd1a6eb35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOTAutograd produced a fx Graph in Aten IR:\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, primals_1: f32[64, 32], primals_2: f32[64], primals_3: f32[8, 32]):\n",
      "        # File: /tmp/ipykernel_7672/678154743.py:7, code: x = self.fc1(x)\n",
      "        t: f32[32, 64] = torch.ops.aten.t.default(primals_1);  primals_1 = None\n",
      "        addmm: f32[8, 64] = torch.ops.aten.addmm.default(primals_2, primals_3, t);  primals_2 = t = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_7672/678154743.py:8, code: x = x.luis_add(x)\n",
      "        luis_add: f32[8, 64] = torch.ops.aten.luis_add.Tensor(addmm, addmm);  addmm = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_7672/678154743.py:9, code: x = torch.nn.functional.gelu(x)\n",
      "        gelu: f32[8, 64] = torch.ops.aten.gelu.default(luis_add)\n",
      "        return [gelu, primals_3, luis_add]\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dude/src/github.com/pytorch/torch/_functorch/aot_autograd.py:1268: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "\n",
    "def toy_backend(gm, sample_inputs): \n",
    "    def my_compiler(gm, sample_inputs):\n",
    "        # <implement your compiler here>\n",
    "        print(\"AOTAutograd produced a fx Graph in Aten IR:\")\n",
    "        gm.print_readable()\n",
    "        return gm.forward\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        fw_compiler=my_compiler\n",
    "    )\n",
    "\n",
    "torch._dynamo.reset()\n",
    "cmodel = torch.compile(model, backend=toy_backend, dynamic=True)\n",
    "\n",
    "# triggers compilation of forward graph on the first run\n",
    "out = cmodel(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "199f5ca0-0743-4f91-859d-c495a723280c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposed fx Graph in Aten IR:\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, primals_1: f32[64, 32], primals_2: f32[64], primals_3: f32[8, 32]):\n",
      "        # File: /tmp/ipykernel_7672/678154743.py:7, code: x = self.fc1(x)\n",
      "        permute: f32[32, 64] = torch.ops.aten.permute.default(primals_1, [1, 0]);  primals_1 = None\n",
      "        addmm: f32[8, 64] = torch.ops.aten.addmm.default(primals_2, primals_3, permute);  primals_2 = permute = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_7672/678154743.py:8, code: x = x.luis_add(x)\n",
      "        luis_add: f32[8, 64] = torch.ops.aten.luis_add.Tensor(addmm, addmm);  addmm = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_7672/678154743.py:9, code: x = torch.nn.functional.gelu(x)\n",
      "        mul: f32[8, 64] = torch.ops.aten.mul.Tensor(luis_add, 0.5)\n",
      "        mul_1: f32[8, 64] = torch.ops.aten.mul.Tensor(luis_add, 0.7071067811865476)\n",
      "        erf: f32[8, 64] = torch.ops.aten.erf.default(mul_1);  mul_1 = None\n",
      "        add: f32[8, 64] = torch.ops.aten.add.Tensor(erf, 1);  erf = None\n",
      "        mul_2: f32[8, 64] = torch.ops.aten.mul.Tensor(mul, add);  mul = add = None\n",
      "        return [mul_2, luis_add, primals_3]\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dude/src/github.com/pytorch/torch/_functorch/aot_autograd.py:1268: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch._inductor.decomposition import decompositions as default_decompositions\n",
    "\n",
    "decompositions = default_decompositions.copy()\n",
    "\n",
    "def toy_backend(gm, sample_inputs):\n",
    "    def my_compiler(gm, sample_inputs):\n",
    "        # <implement your compiler here>\n",
    "        print(\"Decomposed fx Graph in Aten IR:\")\n",
    "        gm.print_readable()\n",
    "        return gm\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        decompositions=decompositions,\n",
    "        fw_compiler=my_compiler\n",
    "    )\n",
    "\n",
    "torch._dynamo.reset()\n",
    "cmodel = torch.compile(model, backend=toy_backend, dynamic=True)\n",
    "\n",
    "# triggers compilation of forward graph on the first run\n",
    "out = cmodel(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c92c33-0803-4ce1-906c-e00faa1a620f",
   "metadata": {},
   "source": [
    "### Prims IR (https://pytorch.org/docs/master/ir.html#prims-ir)\n",
    "\n",
    "* Explicit type promotion and broadcasting\n",
    "* prims.convert_element_type\n",
    "* prims.broadcast_in_dim\n",
    "* For backends with powerful compiler that can reclaim the performance by fusion, e.g. nvFuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "236f9011-fca0-45d9-a832-4f07962d8ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Further decomposed fx Graph in Prims IR:\n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f32[3], arg1_1: f16[3, 3]):\n",
      "        # File: /tmp/ipykernel_2581/2846203934.py:7, code: return a.luis_add(b)\n",
      "        _to_copy: f32[3, 3] = torch.ops.aten._to_copy.default(arg1_1, dtype = torch.float32);  arg1_1 = None\n",
      "        broadcast_in_dim: f32[3, 3] = torch.ops.prims.broadcast_in_dim.default(arg0_1, [3, 3], [1]);  arg0_1 = None\n",
      "        luis_add: f32[3, 3] = torch.ops.prims.luis_add.default(broadcast_in_dim, _to_copy);  broadcast_in_dim = _to_copy = None\n",
      "        return (luis_add,)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "prims_decomp = torch._decomp.get_decompositions([\n",
    "    torch.ops.aten.luis_add,\n",
    "    torch.ops.aten.expand.default,\n",
    "])\n",
    "\n",
    "def fn(a, b):\n",
    "    return a.luis_add(b)\n",
    "\n",
    "def toy_backend(gm, sample_inputs):\n",
    "    def my_compiler(gm, sample_inputs):\n",
    "        # <implement your compiler here>\n",
    "        print(\"Further decomposed fx Graph in Prims IR:\")\n",
    "        gm.print_readable()\n",
    "        return gm\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        decompositions=prims_decomp,\n",
    "        fw_compiler=my_compiler\n",
    "    )\n",
    "\n",
    "torch._dynamo.reset()\n",
    "fn = torch.compile(backend=toy_backend)(fn)\n",
    "out = fn(torch.rand(3, dtype=torch.float), torch.rand(3, 3, dtype=torch.half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c70a43d-7a90-409e-862c-7169c6fd088f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aot_ts_nvfuser', 'cudagraphs', 'inductor', 'ipex', 'nvprims_nvfuser', 'onnxrt', 'tvm']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from torch._dynamo.backends.registry import _BACKENDS as BACKENDS\n",
    "from torch._decomp import core_aten_decompositions\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "\n",
    "print(torch._dynamo.list_backends())\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    try:\n",
    "        trt_compiled = BACKENDS['cudagraphs'](gm, example_inputs) #BACKENDS[\"dynamo_minifier_backend\"](gm, example_inputs, \"onnxrt\")\n",
    "        if trt_compiled is not None:\n",
    "            return trt_compiled\n",
    "    except Exception as e:\n",
    "        print(f'Failed to compile with backend=cudagraphs, err={str(e)}')\n",
    "    pass\n",
    "    \n",
    "     # first backend failed, try something else...\n",
    "    try:      \n",
    "        inductor_compiled = BACKENDS['inductor'](gm, example_inputs) #BACKENDS[\"dynamo_minifier_backend\"](gm, example_inputs, \"inductor\")\n",
    "        if inductor_compiled is not None:\n",
    "            return inductor_compiled\n",
    "    except Exception as e:\n",
    "        print(f'Failed to compile with backend=inductor, err={str(e)}')\n",
    "        pass\n",
    "    \n",
    "    gm.print_readable()\n",
    "    return gm.forward\n",
    "\n",
    "decompositions = core_aten_decompositions()\n",
    "decompositions.update(\n",
    "    torch._decomp.get_decompositions([\n",
    "        torch.ops.aten.addmm,\n",
    "    ])\n",
    ")\n",
    "\n",
    "def toy_backend_2(gm, sample_inputs):\n",
    "    def my_compiler_2(gm, sample_inputs):\n",
    "        try:\n",
    "            trt_compiled = BACKENDS['onnxrt'](gm, sample_inputs) #BACKENDS[\"dynamo_minifier_backend\"](gm, sample_inputs, \"onnxrt\")\n",
    "            if trt_compiled is not None:\n",
    "                print('Going with onnxrt...')\n",
    "                return trt_compiled\n",
    "        except Exception as e:\n",
    "            print(f'Failed to compile with backend=onnxrt, err={str(e)}')\n",
    "        pass\n",
    "\n",
    "         # first backend failed, try something else...\n",
    "        try:      \n",
    "            inductor_compiled = BACKENDS['inductor'](gm, sample_inputs)  #BACKENDS[\"dynamo_minifier_backend\"](gm, sample_inputs, \"inductor\")\n",
    "            if inductor_compiled is not None:\n",
    "                print('Going with inductor...')\n",
    "                return inductor_compiled\n",
    "        except Exception as e:\n",
    "            print(f'Failed to compile with backend=inductor, err={str(e)}')\n",
    "            pass\n",
    "\n",
    "        gm.print_readable()\n",
    "        return gm.forward\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        decompositions=decompositions,\n",
    "        fw_compiler=my_compiler_2\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "409a49fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to compile with backend=onnxrt, err=\n",
      "attribute lookup is not defined on builtin:\n",
      "  File \"<eval_with_key>.726\", line 5\n",
      "def forward(self, arg0_1, arg1_1):\n",
      "    abs_1 = torch.ops.aten.abs.default(arg0_1)\n",
      "            ~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "    add = torch.ops.aten.add.Tensor(abs_1, 1);  abs_1 = None\n",
      "    div = torch.ops.aten.div.Tensor(arg0_1, add);  arg0_1 = add = None\n",
      "\n",
      "Going with inductor...\n",
      "Failed to compile with backend=onnxrt, err=\n",
      "attribute lookup is not defined on builtin:\n",
      "  File \"<eval_with_key>.733\", line 5\n",
      "def forward(self, arg0_1, arg1_1):\n",
      "    mul = torch.ops.aten.mul.Tensor(arg1_1, arg0_1);  arg1_1 = arg0_1 = None\n",
      "          ~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "    return (mul,)\n",
      "\n",
      "Going with inductor...\n",
      "Failed to compile with backend=onnxrt, err=\n",
      "attribute lookup is not defined on builtin:\n",
      "  File \"<eval_with_key>.740\", line 5\n",
      "def forward(self, arg0_1, arg1_1):\n",
      "    mul = torch.ops.aten.mul.Tensor(arg0_1, -1);  arg0_1 = None\n",
      "          ~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "    mul_1 = torch.ops.aten.mul.Tensor(arg1_1, mul);  arg1_1 = mul = None\n",
      "    return (mul_1,)\n",
      "\n",
      "Going with inductor...\n",
      "tensor([  2.2313,   9.6036, -11.4522, -10.2713,  -2.3892,   1.6794,   0.7074,\n",
      "         -5.2095,   4.1134,  -5.4593])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dude/src/github.com/pytorch/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n",
      "/home/dude/src/github.com/pytorch/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n",
      "/home/dude/src/github.com/pytorch/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    }
   ],
   "source": [
    "torch._dynamo.reset()\n",
    "\n",
    "\n",
    "@torch.compile(backend=toy_backend_2)\n",
    "#@torch.compile(backend=my_compiler)\n",
    "def toy_example(a, b):\n",
    "    x = a / (torch.abs(a) + 1)\n",
    "    if b.sum() < 0:\n",
    "        b = b * -1\n",
    "    return x * b\n",
    "\n",
    "a = torch.randn(10)\n",
    "for _ in range(100):\n",
    "    a += toy_example(torch.randn(10), torch.randn(10))\n",
    "\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
