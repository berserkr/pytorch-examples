{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acdb00b0-7032-4133-9cd4-37dddc40a33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch._dynamo\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c8357d-f9a1-4a7f-be01-1aae1c8a76c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "\n",
    "batch_size = 8\n",
    "input = torch.randn(batch_size, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e49cf111-80bc-4074-95a5-19f329d495d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296572f4-0eeb-4ae8-9977-d9d24e1bb7a1",
   "metadata": {},
   "source": [
    "### Invoke `torch.compile` produces a fx graph in Torch IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78f233e-7295-4e7b-9439-7903f3a5a3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamo produced a fx Graph in Torch IR:\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, x : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_8670/1490842273.py:7, code: x = self.fc1(x)\n",
      "        self_fc1 = self.self_fc1(x);  x = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_8670/1490842273.py:8, code: x = torch.nn.functional.gelu(x)\n",
      "        gelu = torch._C._nn.gelu(self_fc1);  self_fc1 = None\n",
      "        return (gelu,)\n",
      "        \n",
      "Notice that sample_inputs is a list of flattened FakeTensor:\n",
      "[FakeTensor(FakeTensor(..., device='meta', size=(s0, 32)), cpu)]\n"
     ]
    }
   ],
   "source": [
    "def toy_backend(gm, sample_inputs):\n",
    "    print(\"Dynamo produced a fx Graph in Torch IR:\")\n",
    "    gm.print_readable()\n",
    "\n",
    "    print(\"Notice that sample_inputs is a list of flattened FakeTensor:\")\n",
    "    print(sample_inputs)\n",
    "    return gm.forward\n",
    "\n",
    "torch._dynamo.reset()\n",
    "cmodel = torch.compile(model, backend=toy_backend, dynamic=True)\n",
    "\n",
    "# triggers compilation of forward graph on the first run\n",
    "out = cmodel(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d8eba-34d4-42a1-9914-32f428403042",
   "metadata": {},
   "source": [
    "## Invoke AOTAutograd, produces forward + backward FX graph in Aten IR\n",
    "* Captures forward + backwards\n",
    "* Lowering from Torch IR to Aten/Prims IR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68d6c5-f52a-471c-bff6-c5127cea5723",
   "metadata": {},
   "source": [
    "### Core Aten IR (https://pytorch.org/docs/master/ir.html#core-aten-ir)\n",
    "\n",
    "* A strict subset of aten operators (< 250) after decompositions\n",
    "* Purely functional (no inputs mutationsï¼‰\n",
    "* Guaranteed metadata information, e.g. dtype and shape propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a66032-61e8-4902-b4b2-056cd1a6eb35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOTAutograd produced a fx Graph in Aten IR:\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, primals_1: f32[64, 32], primals_2: f32[64], primals_3: f32[s0, 32]):\n",
      "        # File: /tmp/ipykernel_8670/1490842273.py:7, code: x = self.fc1(x)\n",
      "        t: f32[32, 64] = torch.ops.aten.t.default(primals_1);  primals_1 = None\n",
      "        addmm: f32[s0, 64] = torch.ops.aten.addmm.default(primals_2, primals_3, t);  primals_2 = t = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_8670/1490842273.py:8, code: x = torch.nn.functional.gelu(x)\n",
      "        gelu: f32[s0, 64] = torch.ops.aten.gelu.default(addmm)\n",
      "        return [gelu, addmm, primals_3]\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1251: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "\n",
    "def toy_backend(gm, sample_inputs): \n",
    "    def my_compiler(gm, sample_inputs):\n",
    "        # <implement your compiler here>\n",
    "        print(\"AOTAutograd produced a fx Graph in Aten IR:\")\n",
    "        gm.print_readable()\n",
    "        return gm.forward\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        fw_compiler=my_compiler\n",
    "    )\n",
    "\n",
    "torch._dynamo.reset()\n",
    "cmodel = torch.compile(model, backend=toy_backend, dynamic=True)\n",
    "\n",
    "# triggers compilation of forward graph on the first run\n",
    "out = cmodel(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199f5ca0-0743-4f91-859d-c495a723280c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposed fx Graph in Aten IR:\n",
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, primals_1: f32[64, 32], primals_2: f32[64], primals_3: f32[s0, 32]):\n",
      "        # File: /tmp/ipykernel_8670/1490842273.py:7, code: x = self.fc1(x)\n",
      "        permute: f32[32, 64] = torch.ops.aten.permute.default(primals_1, [1, 0]);  primals_1 = None\n",
      "        addmm: f32[s0, 64] = torch.ops.aten.addmm.default(primals_2, primals_3, permute);  primals_2 = permute = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_8670/1490842273.py:8, code: x = torch.nn.functional.gelu(x)\n",
      "        mul: f32[s0, 64] = torch.ops.aten.mul.Tensor(addmm, 0.5)\n",
      "        mul_1: f32[s0, 64] = torch.ops.aten.mul.Tensor(addmm, 0.7071067811865476)\n",
      "        erf: f32[s0, 64] = torch.ops.aten.erf.default(mul_1);  mul_1 = None\n",
      "        add: f32[s0, 64] = torch.ops.aten.add.Tensor(erf, 1);  erf = None\n",
      "        mul_2: f32[s0, 64] = torch.ops.aten.mul.Tensor(mul, add);  mul = add = None\n",
      "        return [mul_2, addmm, primals_3]\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1251: UserWarning: Your compiler for AOTAutograd is returning a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch._inductor.decomposition import decompositions as default_decompositions\n",
    "\n",
    "decompositions = default_decompositions.copy()\n",
    "\n",
    "def toy_backend(gm, sample_inputs):\n",
    "    def my_compiler(gm, sample_inputs):\n",
    "        # <implement your compiler here>\n",
    "        print(\"Decomposed fx Graph in Aten IR:\")\n",
    "        gm.print_readable()\n",
    "        return gm\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        decompositions=decompositions,\n",
    "        fw_compiler=my_compiler\n",
    "    )\n",
    "\n",
    "torch._dynamo.reset()\n",
    "cmodel = torch.compile(model, backend=toy_backend, dynamic=True)\n",
    "\n",
    "# triggers compilation of forward graph on the first run\n",
    "out = cmodel(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c92c33-0803-4ce1-906c-e00faa1a620f",
   "metadata": {},
   "source": [
    "### Prims IR (https://pytorch.org/docs/master/ir.html#prims-ir)\n",
    "\n",
    "* Explicit type promotion and broadcasting\n",
    "* prims.convert_element_type\n",
    "* prims.broadcast_in_dim\n",
    "* For backends with powerful compiler that can reclaim the performance by fusion, e.g. nvFuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236f9011-fca0-45d9-a832-4f07962d8ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Further decomposed fx Graph in Prims IR:\n",
      "class <lambda>(torch.nn.Module):\n",
      "    def forward(self, arg0_1: f32[3], arg1_1: f16[3, 3]):\n",
      "        # File: /tmp/ipykernel_8670/2178452752.py:7, code: return a + b\n",
      "        _to_copy: f32[3, 3] = torch.ops.aten._to_copy.default(arg1_1, dtype = torch.float32);  arg1_1 = None\n",
      "        broadcast_in_dim: f32[3, 3] = torch.ops.prims.broadcast_in_dim.default(arg0_1, [3, 3], [1]);  arg0_1 = None\n",
      "        add: f32[3, 3] = torch.ops.prims.add.default(broadcast_in_dim, _to_copy);  broadcast_in_dim = _to_copy = None\n",
      "        return (add,)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "prims_decomp = torch._decomp.get_decompositions([\n",
    "    torch.ops.aten.add,\n",
    "    torch.ops.aten.expand.default,\n",
    "])\n",
    "\n",
    "def fn(a, b):\n",
    "    return a + b\n",
    "\n",
    "def toy_backend(gm, sample_inputs):\n",
    "    def my_compiler(gm, sample_inputs):\n",
    "        # <implement your compiler here>\n",
    "        print(\"Further decomposed fx Graph in Prims IR:\")\n",
    "        gm.print_readable()\n",
    "        return gm\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        decompositions=prims_decomp,\n",
    "        fw_compiler=my_compiler\n",
    "    )\n",
    "\n",
    "torch._dynamo.reset()\n",
    "fn = torch.compile(backend=toy_backend)(fn)\n",
    "out = fn(torch.rand(3, dtype=torch.float), torch.rand(3, 3, dtype=torch.half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c70a43d-7a90-409e-862c-7169c6fd088f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
