{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56b2a71-124c-49cd-9dae-b96e2caf3f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch._dynamo\n",
    "from torchvision import models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188ff56b-fbb1-4c22-a4da-2ead661d8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x**4 + x**3 + x**2 + x\n",
    "\n",
    "x = torch.rand(10000,10000, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4ed7127-ef22-4b9a-a8f5-95e7b3bf416f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us     144.535ms        80.60%     144.535ms     144.535ms             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.434ms         6.93%      12.434ms       2.487ms             5  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       5.020ms         2.80%       5.020ms       1.673ms             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.976ms         2.77%       4.976ms       1.659ms             3  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.311ms         1.85%       3.311ms       1.655ms             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.292ms         1.84%       3.292ms       1.646ms             2  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       1.665ms         0.93%       1.665ms       1.665ms             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.657ms         0.92%       1.657ms       1.657ms             1  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.641ms         0.92%       1.641ms       1.641ms             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     789.000us         0.44%     789.000us     789.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 179.561ms\n",
      "Self CUDA time total: 179.323ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-03-24 23:05:33 2552:2552 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-03-24 23:05:33 2552:2552 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-03-24 23:05:33 2552:2552 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "with profile(activities=[ProfilerActivity.CUDA],\n",
    "            ) as prof:\n",
    "    out = model(x).sum().backward()\n",
    "\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=10))\n",
    "\n",
    "prof.export_chrome_trace(\"no_compile_trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd457346-3bbf-4cdb-ab58-278a90388f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch._dynamo.reset()\n",
    "compiled_model = torch.compile(model,options={'trace.graph_diagram':True,\n",
    "                                'trace.enabled':True})\n",
    "# out = compiled_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14765a05-45ff-4d6a-9d05-b1ebfa09f5f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-03-24 23:05:33 2552:2552 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "[2023-03-24 23:05:35,300] torch._inductor.debug: [WARNING] model__0_forward_1 debug trace: /pytorch-examples/friday24/torch_compile_debug/run_2023_03_24_23_05_35_131295-pid_2552/aot_torchinductor/model__0_forward_1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing FX graph to file: /pytorch-examples/friday24/torch_compile_debug/run_2023_03_24_23_05_35_131295-pid_2552/aot_torchinductor/model__0_forward_1.0/graph_diagram.svg\n",
      "Writing FX graph to file: /pytorch-examples/friday24/torch_compile_debug/run_2023_03_24_23_05_35_131295-pid_2552/aot_torchinductor/model__0_backward_2.1/graph_diagram.svg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-24 23:05:35,448] torch._inductor.debug: [WARNING] model__0_backward_2 debug trace: /pytorch-examples/friday24/torch_compile_debug/run_2023_03_24_23_05_35_131295-pid_2552/aot_torchinductor/model__0_backward_2.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Memcpy DtoH (Device -> Pageable)         0.00%       0.000us         0.00%       0.000us       0.000us     144.811ms        96.16%     144.811ms     144.811ms             1  \n",
      "                                       triton__0d1d2d3d         0.00%       0.000us         0.00%       0.000us       0.000us       2.512ms         1.67%       2.512ms       2.512ms             1  \n",
      "                                         triton__0d1d2d         0.00%       0.000us         0.00%       0.000us       0.000us       1.677ms         1.11%       1.677ms       1.677ms             1  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     807.000us         0.54%     807.000us     807.000us             1  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     790.000us         0.52%     790.000us     790.000us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                                  cudaStreamIsCapturing         0.02%      26.000us         0.02%      26.000us       0.722us       0.000us         0.00%       0.000us       0.000us            36  \n",
      "                                  cudaDeviceSynchronize         0.03%      49.000us         0.03%      49.000us      16.333us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                        cudaMemsetAsync         0.01%      12.000us         0.01%      12.000us      12.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 147.835ms\n",
      "Self CUDA time total: 150.600ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-03-24 23:05:36 2552:2552 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-03-24 23:05:36 2552:2552 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "with profile(activities=[ProfilerActivity.CUDA],\n",
    "            ) as prof:\n",
    "    out = compiled_model(x).sum().backward()\n",
    "\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=10))\n",
    "\n",
    "prof.export_chrome_trace(\"compiled_trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd2c29d3-dc6d-4180-8077-0debc2d0da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fx import passes, symbolic_trace\n",
    "model_sym = symbolic_trace(model)\n",
    "\n",
    "g = passes.graph_drawer.FxGraphDrawer(model_sym, 'fn')\n",
    "with open(\"forward.svg\", \"wb\") as f:\n",
    "    f.write(g.get_dot_graph().create_svg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f98b5bf6-9785-409b-a9ea-804ef24c9c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1251: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1251: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "\n",
    "def toy_backend(gm, sample_inputs): \n",
    "    def fw(gm, sample_inputs):\n",
    "        return gm.forward\n",
    "    \n",
    "    def bw(gm, sample_inputs):\n",
    "        g = passes.graph_drawer.FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"backward.svg\", \"wb\") as f:\n",
    "            f.write(g.get_dot_graph().create_svg())\n",
    "        return gm.forward\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        fw_compiler=fw,\n",
    "        bw_compiler=bw\n",
    "    )\n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_model = torch.compile(model, backend=toy_backend)\n",
    "\n",
    "out = compiled_model(x).sum().backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9077a-0681-4355-9c07-be00f2ab53f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
